<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Generation of Musical Timbres using a Text-Guided Diffusion Model">
  <meta property="og:title" content="Generation of Musical Timbres using a Text-Guided Diffusion Model"/>
  <meta property="og:description" content="Project page for Generation of Musical Timbres using a Text-Guided Diffusion Model"/>
  <!-- TODO: change this -->
  <meta property="og:url" content="https://yininghase.github.io/multi-agent-control/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="images/overview.jpg" /> -->
  <!-- <meta property="og:image:width" content="1245"/> -->
  <!-- <meta property="og:image:height" content="562"/> -->


  <!-- <meta name="twitter:title" content="Multi Agent Navigation in Unconstrained Environments using a Centralized Attention based Graphical Neural Network Controller"> -->
  <!-- <meta name="twitter:description" content="Multi Agent Navigation using a U-Attention based Graphical Neural Network"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="images/overview.jpg"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Generation of Musical Timbres using a Text-Guided Diffusion Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Generation of Musical Timbres using a Text-Guided Diffusion Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.icon">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!-- added by Weixuan -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Generation of Musical Timbres using a Text-Guided Diffusion Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/WxuanYuan" target="_blank">Weixuan Yuan<sup>1</sup></a>,</span>
                <span class="author-block">
                <a href="https://cvg.cit.tum.de/members/khamuham" target="_blank">Qadeer Khan<sup>1,2</sup></a>,</span>
                <span class="author-block">
                <a href="https://cvg.cit.tum.de/members/golkov" target="_blank">Vladimir Golkov<sup>1,2</sup></a>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Technical University of Munich<sup>1</sup>, Munich Center for Machine Learning<sup>2</sup>
                <!-- <br>IEEE International Conference on Intelligent Transportation Systems ITSC 2023 -->
              </span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">


<!--              <div class="publication-links">-->
<!--                    &lt;!&ndash; Arxiv PDF link &ndash;&gt;-->
<!--                <span class="link-block">-->
<!--                  <a href="https://ieeexplore.ieee.org/abstract/document/10422072" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->


<!--              &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--              &lt;!&ndash; TODO: change Supplementary !!!!!!!!!!!!!!! &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="supplementary.pdf" target="_blank"-->
<!--                class="external-link button is-normal is-rounded is-dark">-->
<!--                <span class="icon">-->
<!--                  <i class="fas fa-file-pdf"></i>-->
<!--                </span>-->
<!--                <span>Supplementary</span>-->
<!--                </a>-->
<!--              </span>-->

              <!-- Github link -->
              <!-- TODO: change repository -->
              <span class="link-block">
                <a href="https://github.com/WxuanYuan/diffusynth" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
              </span>

              <!-- WebUI link -->
              <!-- TODO: change link and host webUI -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/WeixuanYuan/DiffuSynthV0.2" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>WebAPP</span>
                </a>
              </span>

<!--                &lt;!&ndash; ArXiv abstract Link &ndash;&gt;-->
<!--                &lt;!&ndash; TODO: what ArXiv &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/abs/2307.16727" target="_blank"-->
<!--                class="external-link button is-normal is-rounded is-dark">-->
<!--                <span class="icon">-->
<!--                  <i class="ai ai-arxiv"></i>-->
<!--                </span>-->
<!--                <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->

            </div>
            <p style="color: red;">
            The WebApp will go into inactive sleep after a period of time. Please click 'Restart this Space' to wake it up, which may take up to 5 minutes to restart.
            </p>



          </div>
        </div>
      </div>
    </div>
  </div>
</section>









<!--<section class="hero teaser" id="Video-Introduction">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video poster="" id="tree" autoplay controls muted loop height="100%">-->
<!--        &lt;!&ndash; Your video here &ndash;&gt;-->
<!--        <source src="images/video_introdcution.mp4"-->
<!--        type="video/mp4">-->
<!--      </video>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<!-- Paper abstract -->
<!-- TODO: update abstract -->
<section class="section hero is-small" id="Abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>In recent years, text-to-audio systems have achieved remarkable success, enabling the generation of complete audio segments, directly from text descriptions. While these systems also facilitate music creation for general users, they often limit human creativity and deliberate expression for artists and musicians.
In contrast, this work allows composers, arrangers, and performers to create the basic building blocks for music creation: audio of individual musical notes for use in electronic instruments and DAWs. Through text prompts, the user can specify the timbre characteristics of the audio.
We introduce a system that combines a latent diffusion model and multi-modal contrastive learning to generate musical timbres conditioned on text descriptions. By jointly generating the magnitude and phase of the spectrogram, our method eliminates the need for subsequently running a phase retrieval algorithm, as related methods do.
        </div>
      </div>
    </div>
  </div>
<!--  <br />-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered is-four-fifths">-->
<!--      <div class="column is-four-fifths is-centered">-->
<!--          <img src="images/overview.jpg" alt="MY ALT TEXT"/>-->
<!--      </div> -->
<!--    </div>-->
<!--  </div>-->
</section>



<section>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
      <p>
      The workflow of the proposed method:
      </p>
      </div>
    </div>
  </div>
  <br /> 
  <div class="container is-max-desktop">
    <div class="columns is-centered is-four-fifths">
      <div class="column is-four-fifths is-centered">
        <img src="images/workflow_shortened.png" alt="MY ALT TEXT"/>
      </div> 
    </div>
  </div>
  <br /> 
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
      <p>
        A Gradio webapp implementing the workflow above is hosted <a href="https://huggingface.co/spaces/WeixuanYuan/DiffuSynthV0.2" target="_blank">here</a>. (It may take some time to load.)
      </p>
      </div>
    </div>
  </div>  
  <br /> 
  <br /> 
</section>
<!-- End paper abstract -->



<!--&lt;!&ndash; Table of Contents Section &ndash;&gt;-->
<!--<section class="hero is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title is-3">Navigation</h2>-->
<!--      <ul>-->
<!--        <li><a href="#Text-to-musical-notes-comparison" style="color: blue; text-decoration: underline;">Text-to-Musical-Notes Comparison</a></li>-->
<!--        <li><a id="stableInPaintLink" style="color: blue; text-decoration: underline; cursor: pointer;">StableInPaint Illustration</a></li>-->
<!--      </ul>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->




<section class="hero is-small is-light" id="Text-to-musical-notes-comparison">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">
        Text to musical notes comparison
      </h2>
      <div id="results-carousel" class="carousel results-carousel" data-interval="30">



        <div class="item">
          <h2 class="subtitle has-text-centered">
            Text description: "This is a sound <strong>that features a reed instrument</strong>."
          </h2>

          <h2 class="subtitle has-text-left">
            Samples generated by our framework:
          </h2>

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description2/our framework/t2s_208_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description2/our framework/t2s_209_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description2/our framework/t2s_210_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description2/our framework/t2s_211_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/our framework/t2s_208.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/our framework/t2s_209.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/our framework/t2s_210.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/our framework/t2s_211.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>

          <h2 class="subtitle has-text-left">
            Samples generated by AudioLDM:
          </h2>

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description2/AudioLDM/sample2_index1_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description2/AudioLDM/sample2_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description2/AudioLDM/sample2_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description2/AudioLDM/sample2_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/AudioLDM/sample2_index1.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/AudioLDM/sample2_index2.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/AudioLDM/sample2_index3.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description2/AudioLDM/sample2_index4.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>

        </div>


        <div class="item">
          <h2 class="subtitle has-text-centered">
            Text description: "This is a sound that features a reed instrument <strong>with a dark tone</strong>."
          </h2>

          <h2 class="subtitle has-text-left">
            Samples generated by our framework:
          </h2>

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description3/our framework/t2s_216_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description3/our framework/t2s_217_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description3/our framework/t2s_218_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description3/our framework/t2s_219_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/our framework/t2s_216.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/our framework/t2s_217.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/our framework/t2s_218.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/our framework/t2s_219.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>

          <h2 class="subtitle has-text-left">
            Samples generated by AudioLDM:
          </h2>

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description3/AudioLDM/sample3_index1_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description3/AudioLDM/sample3_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description3/AudioLDM/sample3_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description3/AudioLDM/sample3_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/AudioLDM/sample3_index1.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/AudioLDM/sample3_index2.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/AudioLDM/sample3_index3.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description3/AudioLDM/sample3_index4.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>

        </div>


        <div class="item">
          <h2 class="subtitle has-text-centered">
            Text description: "This is a sound that features a reed instrument with a dark tone<strong>and a long release</strong>."
          </h2>

          <h2 class="subtitle has-text-left">
            Samples generated by our framework:
          </h2>

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description4/our framework/t2s_224_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description4/our framework/t2s_225_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description4/our framework/t2s_226_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description4/our framework/t2s_227_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/our framework/t2s_224.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/our framework/t2s_225.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/our framework/t2s_226.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/our framework/t2s_227.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>

          <h2 class="subtitle has-text-left">
            Samples generated by AudioLDM:
          </h2>

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description4/AudioLDM/sample4_index1_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description4/AudioLDM/sample4_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description4/AudioLDM/sample4_index3_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description4/AudioLDM/sample4_index4_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/AudioLDM/sample4_index1.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/AudioLDM/sample4_index2.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/AudioLDM/sample4_index3.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description4/AudioLDM/sample4_index4.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>

        </div>

                <div class="item">
          <h2 class="subtitle has-text-centered">
            Text description: "This is a sound."
          </h2>

          <h2 class="subtitle has-text-left">
            Samples generated by our framework:
          </h2>

<!--          &lt;!&ndash; Row of Text &ndash;&gt;-->
<!--          <div class="columns is-centered">-->
<!--            <div class="column is-one-quarter has-text-centered">-->
<!--              <h2 class="subtitle">Sample 1</h2>-->
<!--            </div>-->
<!--            <div class="column is-one-quarter has-text-centered">-->
<!--              <h2 class="subtitle">Sample 2</h2>-->
<!--            </div>-->
<!--            <div class="column is-one-quarter has-text-centered">-->
<!--              <h2 class="subtitle">Sample 3</h2>-->
<!--            </div>-->
<!--            <div class="column is-one-quarter has-text-centered">-->
<!--              <h2 class="subtitle">Sample 4</h2>-->
<!--            </div>-->
<!--          </div>-->

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description1/our framework/t2s_200_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description1/our framework/t2s_201_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description1/our framework/t2s_202_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description1/our framework/t2s_203_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/our framework/t2s_200.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/our framework/t2s_201.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/our framework/t2s_202.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/our framework/t2s_203.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>

          <h2 class="subtitle has-text-left">
            Samples generated by AudioLDM:
          </h2>

          <!-- Row of Images -->
          <div class="columns is-centered">
            <div class="column is-one-quarter">
              <img src="text2sound/description1/AudioLDM/sample1_index1_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description1/AudioLDM/sample1_index2_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description1/AudioLDM/sample1_index3_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
            <div class="column is-one-quarter">
              <img src="text2sound/description1/AudioLDM/sample1_index4_spectrogram.png" alt="Example PNG Image" style="width: 200px; height: 200px;">
            </div>
          </div>

          <!-- Row of Audio Players -->
          <div class="columns is-centered">
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/AudioLDM/sample1_index1.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/AudioLDM/sample1_index2.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/AudioLDM/sample1_index3.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
            <div class="column is-one-quarter has-text-centered">
              <audio controls>
                <source src="text2sound/description1/AudioLDM/sample1_index4.wav" type="audio/wav">
                Your browser does not support the audio element.
              </audio>
            </div>
          </div>
        </div>


      </div>
      <h2>
        Both frameworks faithfully interpret simple text descriptions, such as specifying a particular instrument like ``reed.'' However, when additional constraints are added to the text, our framework produces more precise outputs. For instance, it effectively reduces high-frequency overtones in response to the extended prompt "with a dark tone" and enhances the tail end of the sound according to the extended prompt "and a long release."
      </h2>
    </div>
  </div>
  <br /> 
</section>


<section class="hero is-small"  id="Sampling-with-Different-Guidance-Scale">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">
          Sampling with Different Guidance Scale (\( w \))
      </h2>
      <div class="columns is-centered is-four-fifths">
        <div class="column is-one-tenth has-text-centered">
          <img src="images/CFG.png" alt="Image CFG" style="width: 100%; height: auto;">
        </div>
      </div>

      <!-- Horizontal row with 4 audio players -->
      <div class="columns">
        <div class="column is-one-quarter has-text-centered">
          <audio controls style="width: 100%;">
            <source src="CFG_samples/cs0.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-one-quarter has-text-centered">
          <audio controls style="width: 100%;">
            <source src="CFG_samples/cs1.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-one-quarter has-text-centered">
          <audio controls style="width: 100%;">
            <source src="CFG_samples/cs2.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-one-quarter has-text-centered">
          <audio controls style="width: 100%;">
            <source src="CFG_samples/cs3.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <h2>
        Results of conditioned sampling with varying guidance scales \( w \). As guidance scale increases, more high-frequency components are introduced into the spectrogram in line with the text description.
      </h2>
    </div>
  </div>
  <br /> 
</section>


<section class="hero is-small is-light"  id="Audio-Style-Transfer">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">
          Audio Style Transfer
      </h2>

      <!-- Row above the image: 4 audio players -->
      <div class="columns is-centered">
        <!-- Offset on the left to create space -->
        <div class="column is-offset-2 is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_1.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_2.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_3.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_5.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <!-- Offset on the right to create space -->
        <div class="column is-offset-1"></div>
      </div>

      <!-- Row with image and audio on the sides -->
      <div class="columns is-vcentered is-centered is-four-fifths">
        <!-- Left side audio -->
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_0.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>

        <!-- Center image (60% width) -->
        <div class="column is-8 has-text-centered">
          <img src="images/style_transfer.png" alt="Image CFG" style="width: 100%; height: auto;">
        </div>

        <!-- Right side audio -->
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_11.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <!-- Row above the image: 4 audio players -->
      <div class="columns is-centered">
        <!-- Offset on the left to create space -->
        <div class="column is-offset-2 is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_6.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_7.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_8.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="column is-2 has-text-centered">
          <audio controls style="width: 100%;">
            <source src="style_transfer/st_10.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <!-- Offset on the right to create space -->
        <div class="column is-offset-1"></div>
      </div>

      <h2>
        Smooth transitions in timbre are achieved by altering the guidance scale \( w \) (upper row), or by changing the noising strength through the initial time step \( T_0 \) (lower row). The text description is ``guitar''.
      </h2>
    </div>
  </div>
  <br />
</section>



<!--<section class="hero is-small" id="Duration-Adjustment-Comparisons">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title is-3">-->
<!--          Duration Adjustment Comparisons-->
<!--      </h2>-->

<!--      <div id="results-carousel" class="carousel results-carousel" data-interval="30">-->


<!--        <div class="item">-->
<!--          <h2 class="subtitle has-text-left">-->
<!--            Sample1:-->
<!--          </h2>-->

<!--          <div class="columns is-multiline">-->

<!--            &lt;!&ndash; Column 1: Image (居中) &ndash;&gt;-->
<!--            <div class="column is-one-tenth has-text-centered">-->
<!--              <img src="images/dc/sample2.png" alt="Image 1" style="width: 40%; height: auto;">-->
<!--            </div>-->

<!--            &lt;!&ndash; Column 2: 6段 Audio，垂直排列 &ndash;&gt;-->
<!--            <div class="column is-one-tenth">-->
<!--              <div class="content">-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample2/original/dc_36.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample2/envelope/dc_41.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample2/time_stretching/dc_54.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample2/RePaint_high_noise/dc_42.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample2/RePaint_low_noise/dc_43.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample2/StablePaint/dc_44.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->




<!--        <div class="item">-->
<!--          <h2 class="subtitle has-text-left">-->
<!--            Sample2:-->
<!--          </h2>-->

<!--          <div class="columns is-multiline">-->

<!--            &lt;!&ndash; Column 1: Image (居中) &ndash;&gt;-->
<!--            <div class="column is-one-tenth has-text-centered">-->
<!--              <img src="images/dc/sample5.png" alt="Image 1" style="width: 40%; height: auto;">-->
<!--            </div>-->

<!--            &lt;!&ndash; Column 2: 6段 Audio，垂直排列 &ndash;&gt;-->
<!--            <div class="column is-one-tenth">-->
<!--              <div class="content">-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample5/original/dc_55.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample5/envelope/dc_60.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample5/time_stretching/dc_65.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample5/RePaint_high_noise/dc_61.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample5/RePaint_low_noise/dc_62.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample5/StablePaint/dc_63.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->




<!--        <div class="item">-->
<!--          <h2 class="subtitle has-text-left">-->
<!--            Sample3:-->
<!--          </h2>-->

<!--          <div class="columns is-multiline">-->

<!--            &lt;!&ndash; Column 1: Image (居中) &ndash;&gt;-->
<!--            <div class="column is-one-tenth has-text-centered">-->
<!--              <img src="images/dc/sample4.png" alt="Image 1" style="width: 40%; height: auto;">-->
<!--            </div>-->

<!--            &lt;!&ndash; Column 2: 6段 Audio，垂直排列 &ndash;&gt;-->
<!--            <div class="column is-one-tenth">-->
<!--              <div class="content">-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample4/original/dc_18.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample4/envelope/dc_23.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample4/time_stretching/dc_50.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample4/RePaint_high_noise/dc_24.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample4/RePaint_low_noise/dc_25.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample4/StablePaint/dc_26.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->


<!--        <div class="item">-->
<!--          <h2 class="subtitle has-text-left">-->
<!--            Sample4:-->
<!--          </h2>-->

<!--          <div class="columns is-multiline">-->

<!--            &lt;!&ndash; Column 1: Image (居中) &ndash;&gt;-->
<!--            <div class="column is-one-tenth has-text-centered">-->
<!--              <img src="images/dc/sample1.png" alt="Image 1" style="width: 40%; height: auto;">-->
<!--            </div>-->

<!--            &lt;!&ndash; Column 2: 6段 Audio，垂直排列 &ndash;&gt;-->
<!--            <div class="column is-one-tenth">-->
<!--              <div class="content">-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample1/original/dc_0.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample1/envelope/dc_5.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample1/time_stretching/dc_46.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample1/RePaint_high_noise/dc_6.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample1/RePaint_low_noise/dc_7.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample1/StablePaint/dc_8.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->




<!--        <div class="item">-->
<!--          <h2 class="subtitle has-text-left">-->
<!--            Sample5:-->
<!--          </h2>-->

<!--          <div class="columns is-multiline">-->

<!--            &lt;!&ndash; Column 1: Image (居中) &ndash;&gt;-->
<!--            <div class="column is-one-tenth has-text-centered">-->
<!--              <img src="images/dc/sample3.png" alt="Image 1" style="width: 40%; height: auto;">-->
<!--            </div>-->

<!--            &lt;!&ndash; Column 2: 6段 Audio，垂直排列 &ndash;&gt;-->
<!--            <div class="column is-one-tenth">-->
<!--              <div class="content">-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample3/original/dc_9.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample3/envelope/dc_14.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample3/time_stretching/dc_48.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample3/RePaint_high_noise/dc_15.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample3/RePaint_low_noise/dc_16.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--                <audio controls style="width: 100%; margin-bottom: 60px;">-->
<!--                  <source src="duration_adjustment_examples/lengthen 3s notes to 5s/sample3/StablePaint/dc_17.wav" type="audio/wav">-->
<!--                  Your browser does not support the audio element.-->
<!--                </audio>-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->


<!--      </div>-->
<!--      <h2>-->
<!--        Comparison of different methods for extending 3-second musical notes to 5 seconds. Our method surpasses baselines in maintaining the similarity of timbre  (marked in blue), vibrato frequency (white), and the release stage (green) with the original samples, as well as consistency of timbre across the time domain (yellow). The envelope approach is not applicable for extending audio. Vibrato frequency refers to the rate at which the pitch of a note varies. The release stage is the period after a note stops being played. For example, after 5 seconds.-->
<!--      </h2>-->
<!--      <h2>-->
<!--        Please click <span id="toggleWord" style="color: blue; text-decoration: underline; cursor: pointer;">here</span> for technical details.-->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--  <br />-->
<!--</section>-->




<!--&lt;!&ndash; Section with a slider (carousel), initially hidden &ndash;&gt;-->
<!--<section id="stableInPaintSection" class="hero is-light" style="display: none; margin-top: 20px;">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title is-3">StableInPaint Illustration</h2>-->

<!--      <div class="carousel-item">-->
<!--        <div class="columns is-centered">-->
<!--          <div class="column is-half">-->
<!--            <div class="content">-->
<!--              <figure class="image">-->
<!--                <img src="images/StableInPaint.png" alt="Image for StableInPaint" style="width: 120%; height: auto;">-->
<!--              </figure>-->
<!--              <figcaption>-->
<!--                A single-step visualization of the diffusion-based inpainting with the proposed dynamic mask schedule.-->
<!--                The objective is to produce musical notes of varying lengths with consistent timbre. The outcome \( z_t \) at step \( t \),-->
<!--                for \( t \) ranging from \( T - 1 \) to 0, results from a combination of two sources:-->
<!--                1) The latent representation \( \tilde{z}_t \) denoised from the outcome \( z_{t+1} \) of the previous step using the-->
<!--                trained diffusion model with an empty string as text description.-->
<!--                2) The reshaped (see Fig. 5) and noise-added representation \( u'_t \) obtained from the latent representation \( u \)-->
<!--                of the specific sound. Both sources are merged based on the mask \( M_t \) of the current time step, defined by-->
<!--                a dynamic mask schedule. This method overcomes existing methods' disadvantages, such as mismatched release stages,-->
<!--                distortion, and variations in vibrato frequency.-->
<!--              </figcaption>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->

<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<!--<script>-->
<!--  // JavaScript to toggle the section display when clicking on the word-->
<!--  document.getElementById("toggleWord").addEventListener("click", function() {-->
<!--    var section = document.getElementById("stableInPaintSection");-->

<!--    if (section.style.display === "none") {-->
<!--      section.style.display = "block";-->
<!--    } else {-->
<!--      section.style.display = "none";-->
<!--    }-->
<!--  });-->
<!--</script>-->









<!-- Section with a slider (carousel) -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Example music tracks synthesized by our workflow</h2>

      <!-- Carousel slider part -->
      <div id="carousel-demo" class="carousel">

        <!-- Slide 1 -->
        <div class="carousel-item">
          <div class="columns is-centered">
            <div class="column is-half">
              <div class="content">
                <h3 class="subtitle has-text-left">Example 1</h3>
                <figure class="image">
                  <img src="music_examples/sample3_spectrogram.png" alt="Image for Slide 3" style="width: 100%; height: auto;">
                </figure>
                <audio controls style="width: 100%; margin-top: 10px;">
                  <source src="music_examples/sample3.wav" type="audio/wav">
                  Your browser does not support the audio element.
                </audio>
              </div>
            </div>
          </div>
        </div>

        <!-- Slide 2 -->
        <div class="carousel-item">
          <div class="columns is-centered">
            <div class="column is-half">
              <div class="content">
                <h3 class="subtitle has-text-left">Example 2</h3>
                <figure class="image">
                  <img src="music_examples/sample1_spectrogram.png" alt="Image for Slide 1" style="width: 100%; height: auto;">
                </figure>
                <audio controls style="width: 100%; margin-top: 10px;">
                  <source src="music_examples/sample1.wav" type="audio/wav">
                  Your browser does not support the audio element.
                </audio>
              </div>
            </div>
          </div>
        </div>

        <!-- Slide 3 -->
        <div class="carousel-item">
          <div class="columns is-centered">
            <div class="column is-half">
              <div class="content">
                <h3 class="subtitle has-text-left">Example 3</h3>
                <figure class="image">
                  <img src="music_examples/sample2_spectrogram.png" alt="Image for Slide 2" style="width: 100%; height: auto;">
                </figure>
                <audio controls style="width: 100%; margin-top: 10px;">
                  <source src="music_examples/sample2.wav" type="audio/wav">
                  Your browser does not support the audio element.
                </audio>
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>














<!--&lt;!&ndash;BibTex citation &ndash;&gt;-->
<!--<section class="section" id="BibTeX">-->
<!--  <div class="container is-max-desktop content">-->
<!--    <h2 class="title">BibTeX</h2>-->
<!--    <pre><code>-->
<!--      @INPROCEEDINGS{10422072,-->
<!--        author={Ma, Yining and Khan, Qadeer and Cremers, Daniel},-->
<!--        booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, -->
<!--        title={Multi Agent Navigation in Unconstrained Environments using a Centralized Attention based Graphical Neural Network Controller}, -->
<!--        year={2023},-->
<!--        volume={},-->
<!--        number={},-->
<!--        pages={2893-2900},-->
<!--        keywords={Training;Codes;Navigation;Neural networks;Training data;Data models;Optimization},-->
<!--        doi={10.1109/ITSC57777.2023.10422072}}</code></pre>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash;End BibTex citation &ndash;&gt;-->







<section class="hero is-small">
  <div class="hero-body">
    <h2 class="title is-3">Insights of the model</h2>
    <div class="container" style="display: flex; flex-direction: column; align-items: center;">

      <!-- 图片及注释 -->
      <div class="column is-four-fifths is-centered" style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/model_architecture.png" alt="Architecture Overview" />
        <!-- 图片注释 -->
        <div class="content" style="text-align: justify; max-width: 800px; margin-top: 10px; font-size: 14px;">
          <p>
            Architecture overview of our framework for fixed-length music note generation.
            It combines multi-modal contrastive learning and latent diffusion models.
            STFT+ and ISTFT+ represent the non-trainable time-frequency domain transformations of audio signals \(S\).
            (see Spectral Representation Section) A pretrained LLM is used to augment labels such as "bright, guitar"
            from the NSynth dataset to diverse text descriptions. The training is divided into three phases:
          </p>
          <ol>
            <li>
              A <strong>VQ-GAN</strong> (in yellow) is trained as an autoencoder for the spectral representation of real samples.
              Its discriminator \(D\) is trained to distinguish spectral representations of real samples (i.e.~\(x\) for all training samples)
              from those of generated samples (i.e.~\(\hat{x}\) for all training samples). The encoder, decoder, and quantizer are trained to fool
              the discriminator, i.e.~to produce realistic~\(\hat{x}\).
            </li>
            <li>
              A <strong>text encoder</strong> (pretrained using <a href="https://ieeexplore.ieee.org/abstract/document/10095889/" class="citation">CLAP</a>) and a timbre encoder (both shown in green) are trained to map
              text descriptions and the timbre representation \(\hat{z}\) into a unified embedding space via contrastive learning.
            </li>
            <li>
              A <strong>diffusion model</strong> (in blue) is trained to produce latent representations conditioned by the text embeddings.
              During the inference stage, the output of the diffusion model is passed to the VQ-GAN decoder.
            </li>
          </ol>
        </div>
      </div>

      <!-- 正文 -->
      <div class="content" style="text-align: justify; max-width: 800px; margin-top: 30px; font-size: 16px;">
        <!-- Subsection: Text-conditioned Note Generation -->
        <h3 class="title is-4" style="text-align: left;">Text-conditioned Note Generation</h3>
        <h4 class="subtitle is-5" style="text-align: left;">Latent Representation on Notes:</h4>
        <p>
          We train the diffusion model on the lower-dimensional latent representation of the audio to expedite training.
          The audio is first converted to a spectral <em>image</em> representation.
          The Short-Time Fourier Transform (STFT) enables the conversion of an audio signal \(s\) into its magnitude and phase,
          allowing for the near-lossless reconstruction of the original audio signal through the Inverse STFT (ISTFT).
          Based on the STFT, we encode the audio signal \(s\) into a spectral representation \(x \in \mathbb{R}^{3 \times H \times W}\),
          where the three channels correspond to log-magnitude, sine phase, and cosine phase, respectively.
        </p>
        <p>
          We do not use mel-scaled spectrograms due to their tendency to compress high-frequency information, which is detrimental to high-quality music synthesis.
        </p>
        <p>
          This spectral representation is compressed to and reconstructed from a latent representation
          \(\hat{z} \in \mathbb{R}^{C \times \frac{H}{r} \times \frac{W}{r}}\), where \(C\) represents the number of channels,
          and \(r\) denotes the spatial compression scale, via a <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=" class="citation">VQ-GAN</a>.
          As depicted in the yellow section of the Figure above,
          the VQ-GAN features an encoder-decoder architecture, employing convolutional and transposed convolutional layers
          with a stride of 2 for spatial downsampling and upsampling, respectively. The <a href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html" class="citation">quantizer</a> assigns each element
          of the spatial dimensions to its nearest discrete value in a codebook.
        </p>
        <p>
          Additionally, the VQ-GAN has a discriminator that distinguishes between the spectral representation of real samples
          from the training set and those reconstructed by the decoder. This is trained adversarially against the encoder, decoder,
          and quantizer.
        </p>

        <!-- Subsection: Contrastive Representation Learning -->
        <h4 class="subtitle is-5" style="text-align: left;">Contrastive Representation Learning:</h4>
        <p>
          The multi-modal nature of our approach necessitates a shared representation between text and timbre.
          To ensure this, we train a timbre-encoder and a text-encoder, which respectively map the latent audio representation
          and text descriptions to their corresponding embeddings within a unified latent space.
        </p>
        <p>
          This is achieved using contrastive loss by enhancing the cosine similarity between the embeddings of the text and timbre
          within the same sample pair while promoting differentiation among different samples within the batch.
          The approach takes inspiration from the <a href="https://proceedings.neurips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html" class="citation">multi-class N-pair loss</a> utilized in <a href="https://proceedings.mlr.press/v139/radford21a" class="citation">CLIP</a>,
          used for matching texts with images rather than audio.
        </p>
        <p>
          The text-encoder is initialized with pretrained <a href="https://ieeexplore.ieee.org/abstract/document/10095889/" class="citation">CLAP</a> parameters, where CLAP is a multimodal pre-trained model
          that bridges audio and text. In contrast, the timbre-encoder undergoes preliminary classification pre-training using the labels
          from the NSynth dataset, before the joint training. We chose to fine-tune the model rather than using CLAP directly to better
          align text and musical note features, rather than general sounds.
        </p>
        <p>
          Uniquely for this task, since text descriptions are derived from labels, the dataset includes notes with identical labels,
          resulting in semantically similar text descriptions for different samples.
          When optimizing the batch-constructed symmetric cross-entropy loss, samples with similar text descriptions may be erroneously
          considered negative pairs, which impedes model convergence.
          Therefore, we avoid the co-existence of audio samples with <em>exactly</em> identical labels within a single batch during training
          through data filtering.
        </p>
        <p>
          More details on spectral representation, prompt engineering, learning objectives, and ablation studies using CLAP as the text encoder
          can be found in the supplementary material.
        </p>
      </div>


      <div class="content" style="text-align: justify; max-width: 800px; margin-top: 30px; font-size: 16px;">
        <h4 class="subtitle is-5" style="text-align: left;">Denoising Diffusion on Latent Representations</h4>
        <p>
          Given pairs of latent representations and text embeddings \( (\hat{z}, e^{t}) \), we train a Denoising Diffusion Probabilistic Model (DDPM)
          \( p_{\theta}(\hat{z}|e^{t}) \) that closely replicates the conditioned distribution \( q(\hat{z}|e^{t}) \).
        </p>
        <p>
          <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" class="citation">[DDPM]</a> engages in an iterative forward process defined on time steps \(t \in [1, \ldots, T]\), wherein noise \(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})\)
          is incrementally added to true data points \(z_0:=\hat{z}\). This procedure is grounded on a predetermined noise schedule specified by
          \(0 < \beta_1 < \cdots < \beta_t < \cdots < \beta_T < 1\), and is delineated as a Markov Chain in the following manner.
        </p>
        <p>
          <strong>Forward Process Equations:</strong>
        </p>
        <p>
          \( q(z_t | z_{t-1}) := \mathcal{N}(z_t; \sqrt{\alpha_t} z_{t-1}, \beta_t \mathbf{I}), \) <br>
          \( q(z_t | z_0) := \mathcal{N}(z_t; \sqrt{\overline{\alpha}_t} z_0, (1 - \overline{\alpha}_t) \boldsymbol{\epsilon}), \)
        </p>
        <p>
          where \( \alpha_{t} = 1-\beta_{t} \) and \( \bar{\alpha}_t := \prod_{s=1}^{t} \alpha_s \). The reverse process represents the inverse mechanism, delineating a generative model,
          which reconstructs the original data points from the pure Gaussian noise. Specifically, in the task at hand, we are interested in the conditional joint distribution, defined as:
        </p>
        <p>
          \( p_{\theta}(z_{0:T}|e^t) := p(z_T) \prod_{t=1}^{T} p_{\theta}(z_{t-1}|z_t,e^t), \)
        </p>
        <p>
          where \( p(z_T) := \mathcal{N}(0, I) \) and \( e^t \) is the text embedding. The intermediate transitions are parameterized by estimations of a neural network
          \( \boldsymbol{\epsilon}_{\theta} \):
        </p>
        <p>
          \( p_{\theta}(z_{t-1}|z_t,e^t) := \mathcal{N}(z_{t-1}; \mu_{\theta}(z_t, t, e^t), \frac{1 - \hat{\alpha}_{t-1} \beta_{t}}{1 - \hat{\alpha}_{t}} I), \) <br>
          \( \mu_{\theta}(z_t, t, e^t) := \frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{\beta_t}{\sqrt{1 - \hat{\alpha}_t}} \boldsymbol{\epsilon}_{\theta}(z_t, t, e^t) \right). \)
        </p>
        <p>
          The neural network is trained to estimate the weighted noise by minimizing an adapted version of the <a href="https://arxiv.org/abs/1312.6114" class="citation">variational lower bound</a> of the negative log-likelihood :
        </p>
        <p>
          \( L_{\textnormal{t}}^{\textnormal{simple}} = \mathbb{E}_{t, x_0, \boldsymbol{\epsilon}} \left[ \left\| \epsilon - \boldsymbol{\epsilon}_{\theta}(z_t, t, e^t) \right\|^2 \right]. \)
        </p>
        <p>
          Furthermore, <a href="https://arxiv.org/abs/2207.12598" class="citation">classifier-free guidance</a> is applied during training, which involves randomly replacing the text embedding \( e^t \) with the embedding of an
          empty string with probability \( p \).
        </p>
      </div>


    </div>
  </div>
</section>




<section class="hero is-light">
  <div class="hero-body">
    <h2 class="title is-3">Model Hyperparameters and Training Configurations</h2>
    <div class="container" style="display: flex; flex-direction: column; align-items: center;">

      <!-- VQ-GAN -->
      <div class="content" style="text-align: justify; max-width: 800px; margin-top: 30px; font-size: 16px;">
        <h4 class="subtitle is-5" style="text-align: left;">VQ-GAN</h4>

        <p>
          <strong>Training Objective:</strong>
        </p>
        <p>
          The objective in training the VQ-GAN involves minimizing a combination of a weighted reconstruction loss \(L_{\textnormal{rec}}\),
          VQ loss \(L_{\textnormal{VQ}}\), and adversarial loss \(L_{\textnormal{adv}}\). Specifically, the weighted reconstruction loss for the spectral representation \(x\)
          is designed channel-wise to achieve combined compression and reconstruction of magnitude and phase information. For the phase information in the latter two channels,
          the Mean Absolute Error (MAE) is utilized, while for the log-magnitude in the first channel, a weighted Mean Absolute Error (wMAE) is employed to assign greater weight
          to lower values, thereby reducing noise in low amplitudes.
        </p>
        <p>
          For a spectral representation sample \(x\) and its reconstruction \(\hat{x}\), the loss for the generator, that is, the encoder-decoder structure, \(L_{\textnormal{G}}\),
          and the discriminator loss \(L_{\textnormal{D}}\) are defined as follows:
        </p>
        <p>
          \(L_{\textnormal{G}}(x, \hat{x}) := L_{\textnormal{rec}}(x, \hat{x}) + w_1 \cdot L_{\textnormal{VQ}} + w_2 \cdot L_{\textnormal{adv}}(\hat{x})\), <br>
          \(L_{\textnormal{rec}}(x, \hat{x}) := \text{wMAE}(x_1, \hat{x}_1) + w_3 \cdot (\text{MAE}(x_2, \hat{x}_2) + \text{MAE}(x_3, \hat{x}_3))\), <br>
          \(\text{wMAE}(x_1, \hat{x}_1) := \text{avg} \left(\frac{\left|\hat{x}_1 - x_1\right|}{\max(x_1, \epsilon)}\right)\), <br>
          \(L_{\textnormal{adv}}(\hat{x}) := -\log D(\hat{x})\), <br>
          \(L_{\textnormal{D}}(x, \hat{x}) := -\log D(x) - \log(1 - D(\hat{x}))\),
        </p>
        <p>
          where \(D\) is the discriminator, \(w_1\), \(w_2\), \(w_3\) are weight parameters, \(x_i\) denotes the \(i\)-th channel of \(x\), and \(\epsilon\) is a hyper-parameter
          that adjusts weights for lower log-magnitude values. In this work, we set \(w_1 = 10\), \(w_2 = 0.1\), \(w_3 = 1\), \(\epsilon = 0.001\).
        </p>

        <p>
          <strong>Architecture:</strong>
        </p>
        <p>
          The VQ-GAN model comprises an encoder, a decoder, a quantization layer, and a discriminator.
        </p>
        <p>
          The encoder and decoder consist of alternating stacks of ResNet blocks, efficient attention modules, and down/up-sampling modules. Each ResNet block combines <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.html" class="citation">group normalization</a>, swish activation, and a 2D convolutional layer with kernel size 3, along with a skip connection that includes a convolution with kernel size 1.
        </p>
        <p>
          The efficient attention module implements the attention mechanism with linear complexity. The down/up-sampling modules are convolution/transposed convolution layers with a stride of 2 and a kernel size of 4. The encoder features 6 ResNet blocks, 2 efficient attention modules, and two down-sampling modules. The input and output channels of the encoder are 3 and 4, while the hidden channels are 80 and 160. The decoder mirrors this structure but replaces down-sampling with up-sampling modules. Notably, the final activation layer of the decoder is specifically designed for generating spectral representations, using <em>softplus</em> activation for the first channel and <em>tanh</em> activation for the remaining two channels.
        </p>
        <p>
          For the <a href="https://proceedings.neurips.cc/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html" class="citation">quantization layer</a>, we employ the implementation based on exponential moving averages. The codebook stores 8192 discrete entries, each with a channel size of 4.
        </p>
        <p>
          The discriminator employs an <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" class="citation">18-layer-ResNet architecture</a>, with the first layer replaced by a 2D convolutional layer that accommodates spectral representation inputs and the last two layers replaced by a binary classifier layer.
        </p>
        <p>
          The VQ-GAN model has a total of 1.5M trainable parameters. It was trained using the Adam optimizer for 80,000 steps, with a batch size of 4 and a learning rate of \(1 \times 10^{-4}\).
        </p>
      </div>



      <div class="content" style="text-align: justify; max-width: 800px; margin-top: 30px; font-size: 16px;">
        <h4 class="subtitle is-5" style="text-align: left;">Contrastive Pretrain</h4>
        <p>
          The timbre-encoder consists of an <a href="https://sophieeunajang.wordpress.com/wp-content/uploads/2020/10/lstm.pdf" class="citation">LSTM</a>, four single-layer classifier heads, and a single-layer projection head.
          The LSTM has a feature dimension of 512 and a hidden dimension of 1024, with three stacked layers.
          It processes the latent representation \(z\) as a sequence of features along the temporal dimension,
          corresponding to the time frames of the spectral representation \(x\). During the pretraining of the timbre-encoder,
          the final feature output by the LSTM is passed to four classifier heads, which predict labels provided by the <a href="https://magenta.tensorflow.org/datasets/nsynth#motivation" class="citation">NSynth</a> dataset.
          When jointly trained with the text-encoder, the LSTM's final feature is fed into the projection head,
          mapping it to a multi-modal feature space with a dimension of 512.
        </p>
        <p>
          The text-encoder utilizes the architecture and pretrained parameters of <a href="https://ieeexplore.ieee.org/abstract/document/10095889/" class="citation">CLAP</a>, along with a projection head.
          The projection head is also single-layered, mapping the extracted text feature of dimension 512 to the multi-modal feature space.
        </p>
        <p>
          The timbre-encoder has 25M trainable parameters and was pretrained using the Adam optimizer for 20,000 steps,
          with a batch size of 64 and a learning rate of \(10^{-3}\). In the joint training phase, it is trained for 600,000 steps
          with a batch size of 16, together with the text-encoder, which has 155M trainable parameters.
          The optimizer used is AdamW, with a learning rate of \(10^{-5}\) and weight decay of \(10^{-3}\) for the text-encoder,
          and a learning rate of \(10^{-4}\) and weight decay of \(10^{-6}\) for the timbre-encoder.
          However, for the projection heads of both, the learning rate is set to \(10^{-4}\) with a weight decay of \(10^{-6}\).
        </p>
      </div>





      <div class="content" style="text-align: justify; max-width: 800px; margin-top: 30px; font-size: 16px;">
        <h4 class="subtitle is-5" style="text-align: left;">Diffusion Model</h4>
        <p>
          The diffusion model shares similarities with the VQ-GAN in its encoder-decoder structure and building components, namely the ResNet blocks and efficient attention modules.
          However, it distinguishes itself by:
        </p>
        <ul>
          <li>Incorporating skip connections between the encoder and decoder</li>
          <li>Adding time embeddings to the feature maps</li>
          <li>Transforming text embeddings and inserting them into the attention modules via cross attention</li>
        </ul>
        <p>
          The total number of time steps \(T\) is set to 1000. A linear Beta schedule is used, with \(\beta_{1}=1 \times 10^{-4}\) and \(\beta_{T}=2 \times 10^{-2}\).
          For classifier-free guidance, the probability of replacing text descriptions with the empty string is \(p=0.1\).
        </p>
        <p>
          The models were trained using the Adam optimizer for 0.3M steps, with a batch size of 8 and a learning rate of \(10^{-4}\).
          The training is conducted on a single NVIDIA T4 GPU, for approximately 70 hours.
        </p>
      </div>


      <div class="content" style="text-align: justify; max-width: 800px; margin-top: 30px; font-size: 16px;">
        <h4 class="subtitle is-5" style="text-align: left;">Baseline Models</h4>

        <p>
          <strong>GAN:</strong> Inspired by <a href="https://arxiv.org/abs/1902.08710" class="citation">GanSynth</a>, we trained a U-Net model using adversarial learning. This aimed to validate the superiority
          of the diffusion model over GANs in generating musical notes. The primary distinction between our framework and the GAN-based approach lies in the
          loss function, while the model architecture and training configurations remain unchanged.
        </p>

        <p>
          <strong>Our Framework with Pretrained CLAP as Text-Encoder:</strong> To assess the benefits of contrastive pretraining, we employed the pre-trained
          CLAP as the text encoder, without fine-tuning it on our dataset. We retrained the diffusion model within our framework, referring to this
          model as <em>Ours_C</em>. The training settings were consistent with those used for <em>Ours</em>.
        </p>

        <p>
          <strong>Our Framework with a Smaller Model Size:</strong> Additionally, we trained a diffusion model with a reduced channel size within our framework,
          referred to as <em>Ours_S</em>. The training settings for this model were consistent with those of <em>Ours</em>.
        </p>

        <p>
          <strong>AudioLDM and Adapted AudioLDM:</strong> We utilized <a href="https://arxiv.org/abs/2301.12503" class="citation"><em>AudioLDM</em></a>, which is pre-trained on a broad range of sound data including
          musical notes, as a baseline. Furthermore, we adapted the AudioLDM diffusion model to our dataset, resulting in <em>AudioLDM_A</em> as another baseline.
          This adaptation was performed using 2 NVIDIA T4 GPUs, involving 0.3M training steps over approximately 90 hours. This process demanded more computational
          resources compared to our framework. The training configuration followed the author's recommendation, with a batch size of 2 and a learning rate of \(10^{-5}\).
        </p>
      </div>







    </div>
  </div>
</section>






<section class="hero is-small">
  <div class="hero-body">
    <h2 class="title is-3">Spectral Representation</h2>
    <div class="container" style="display: flex; flex-direction: column; align-items: center;">

      <!-- 图片及图片注释 -->
      <div class="column is-four-fifths is-centered" style="display: flex; flex-direction: column; align-items: center;">
        <img src="images/spectral_representation.png" alt="Spectral Representation Illustration" />
        <div class="content" style="text-align: justify; max-width: 800px; margin-top: 10px; font-size: 14px;">
          <p>
            Transformation between time signal \(s\) and spectral representation \(x\).
          </p>
        </div>
      </div>

      <!-- 正文 -->
      <div class="content" style="text-align: justify; max-width: 800px; margin-top: 30px; font-size: 16px;">
        <h3 class="title is-4" style="text-align: left;">Pseudo Code for STFT-based and ISTFT-based Transformation</h3>

        <!-- STFT+ Pseudo Code -->
        <div style="margin-top: 20px;">
          <pre style="background-color: #f8f8f8; padding: 15px; border: 1px solid #ddd; overflow-x: auto; font-size: 14px;">
<strong>Algorithm STFT+(s)</strong>
Input: time signal s
Output: spectral representation x, which is a matrix with three channels representing log magnitude, cosine phase, and sine phase

1. Compute complex spectrum matrix D
   D <- STFT(s)

2. Compute magnitude of D
   magnitude <- absolute value of D

3. Compute phase of D
   phase <- angle of D

4. Compute log magnitude
   log_magnitude <- log(1 + magnitude)

5. Compute cosine of phase
   cos_phase <- cosine(phase)

6. Compute sine of phase
   sin_phase <- sine(phase)

7. Encode as three channels, with channel dimension first
   x <- stack(log_magnitude, cos_phase, sin_phase) along axis 0

8. Return x
          </pre>
          <p style="font-style: italic; font-size: 14px; margin-top: 5px;">STFT-based spectral representation encoding pseudo code.</p>
        </div>

        <!-- ISTFT+ Pseudo Code -->
        <div style="margin-top: 20px;">
          <pre style="background-color: #f8f8f8; padding: 15px; border: 1px solid #ddd; overflow-x: auto; font-size: 14px;">
<strong>Algorithm ISTFT+(x)</strong>
Input: x, a spectral representation matrix with three channels representing log magnitude, cosine phase, and sine phase
Output: time signal s

1. Extract channels from x
   log_magnitude <- first channel of x
   cos_phase <- second channel of x
   sin_phase <- third channel of x

2. Invert log magnitude transformation
   magnitude <- exp(log_magnitude) - 1

3. Calculate phase
   phase <- arctan2(sin_phase, cos_phase)

4. Reconstruct the complex spectrum matrix from magnitude and phase
   D <- magnitude * (cos(phase) + i * sin(phase))

4. Reconstruct time signal
   s <- ISTFT(D)

5. Return s
          </pre>
          <p style="font-style: italic; font-size: 14px; margin-top: 5px;">ISTFT-based spectral representation decoding pseudo code.</p>
        </div>

        <p style="margin-top: 20px;">
          As illustrated in the Figure above, and detailed in the pseudocode provided above, we encode the time signal \(s\)
          into a spectral representation \(x\) using STFT and reconstruct \(s\) using ISTFT, along with processing on the spectral data.
          Since \(x\) retains both magnitude and phase information, the representation and reconstruction of the time signal are almost lossless.
        </p>
      </div>
    </div>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> under the license <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. We thank the authors for the open-source code.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
